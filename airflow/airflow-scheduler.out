[[34m2024-11-28T09:32:16.989+0000[0m] {[34mscheduler_job_runner.py:[0m788} INFO[0m - Starting the scheduler[0m
[[34m2024-11-28T09:32:16.990+0000[0m] {[34mscheduler_job_runner.py:[0m795} INFO[0m - Processing each file at most -1 times[0m
[[34m2024-11-28T09:32:16.995+0000[0m] {[34mmanager.py:[0m165} INFO[0m - Launched DagFileProcessorManager with pid: 4295[0m
[[34m2024-11-28T09:32:16.996+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-11-28T09:32:16.999+0000[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone Timezone('UTC')[0m
[2024-11-28T09:32:17.028+0000] {manager.py:411} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2024-11-28T09:37:17.056+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-11-28T09:41:25.477+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: one_task_dag.one_task manual__2024-11-28T09:41:24.452381+00:00 [scheduled]>[0m
[[34m2024-11-28T09:41:25.477+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG one_task_dag has 0/16 running and queued tasks[0m
[[34m2024-11-28T09:41:25.477+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: one_task_dag.one_task manual__2024-11-28T09:41:24.452381+00:00 [scheduled]>[0m
[[34m2024-11-28T09:41:25.479+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='one_task_dag', task_id='one_task', run_id='manual__2024-11-28T09:41:24.452381+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-11-28T09:41:25.479+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'one_task_dag', 'one_task', 'manual__2024-11-28T09:41:24.452381+00:00', '--local', '--subdir', 'DAGS_FOLDER/one_task_dag.py'][0m
[[34m2024-11-28T09:41:25.483+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'one_task_dag', 'one_task', 'manual__2024-11-28T09:41:24.452381+00:00', '--local', '--subdir', 'DAGS_FOLDER/one_task_dag.py'][0m
[[34m2024-11-28T09:41:26.198+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/one_task_dag.py[0m
[[34m2024-11-28T09:41:26.642+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: one_task_dag.one_task manual__2024-11-28T09:41:24.452381+00:00 [queued]> on host codespaces-c5d7e2[0m
[[34m2024-11-28T09:41:27.051+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='one_task_dag', task_id='one_task', run_id='manual__2024-11-28T09:41:24.452381+00:00', try_number=1, map_index=-1)[0m
[[34m2024-11-28T09:41:27.056+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=one_task_dag, task_id=one_task, run_id=manual__2024-11-28T09:41:24.452381+00:00, map_index=-1, run_start_date=2024-11-28 09:41:26.677100+00:00, run_end_date=2024-11-28 09:41:26.778930+00:00, run_duration=0.10183, state=success, executor_state=success, try_number=1, max_tries=0, job_id=3, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-11-28 09:41:25.478298+00:00, queued_by_job_id=2, pid=8729[0m
[[34m2024-11-28T09:41:27.096+0000[0m] {[34mdagrun.py:[0m630} INFO[0m - Marking run <DagRun one_task_dag @ 2024-11-28 09:41:24.452381+00:00: manual__2024-11-28T09:41:24.452381+00:00, state:running, queued_at: 2024-11-28 09:41:24.468668+00:00. externally triggered: True> successful[0m
[[34m2024-11-28T09:41:27.097+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=one_task_dag, execution_date=2024-11-28 09:41:24.452381+00:00, run_id=manual__2024-11-28T09:41:24.452381+00:00, run_start_date=2024-11-28 09:41:25.452289+00:00, run_end_date=2024-11-28 09:41:27.097606+00:00, run_duration=1.645317, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-11-28 09:41:24.452381+00:00, data_interval_end=2024-11-28 09:41:24.452381+00:00, dag_hash=be40981d7d957f045c72d1193cdb44e7[0m
[[34m2024-11-28T09:42:17.084+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-11-28T09:46:49.212+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: two_task_dag.bash_task_0 manual__2024-11-28T09:46:48.352196+00:00 [scheduled]>[0m
[[34m2024-11-28T09:46:49.213+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG two_task_dag has 0/16 running and queued tasks[0m
[[34m2024-11-28T09:46:49.213+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: two_task_dag.bash_task_0 manual__2024-11-28T09:46:48.352196+00:00 [scheduled]>[0m
[[34m2024-11-28T09:46:49.215+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='two_task_dag', task_id='bash_task_0', run_id='manual__2024-11-28T09:46:48.352196+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-11-28T09:46:49.215+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'two_task_dag', 'bash_task_0', 'manual__2024-11-28T09:46:48.352196+00:00', '--local', '--subdir', 'DAGS_FOLDER/two_task_dag.py'][0m
[[34m2024-11-28T09:46:49.220+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'two_task_dag', 'bash_task_0', 'manual__2024-11-28T09:46:48.352196+00:00', '--local', '--subdir', 'DAGS_FOLDER/two_task_dag.py'][0m
[[34m2024-11-28T09:46:49.961+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/two_task_dag.py[0m
[[34m2024-11-28T09:46:50.419+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: two_task_dag.bash_task_0 manual__2024-11-28T09:46:48.352196+00:00 [queued]> on host codespaces-c5d7e2[0m
[[34m2024-11-28T09:46:50.931+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='two_task_dag', task_id='bash_task_0', run_id='manual__2024-11-28T09:46:48.352196+00:00', try_number=1, map_index=-1)[0m
[[34m2024-11-28T09:46:50.934+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=two_task_dag, task_id=bash_task_0, run_id=manual__2024-11-28T09:46:48.352196+00:00, map_index=-1, run_start_date=2024-11-28 09:46:50.454136+00:00, run_end_date=2024-11-28 09:46:50.558787+00:00, run_duration=0.104651, state=success, executor_state=success, try_number=1, max_tries=0, job_id=4, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator, queued_dttm=2024-11-28 09:46:49.213976+00:00, queued_by_job_id=2, pid=11038[0m
[[34m2024-11-28T09:46:50.984+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: two_task_dag.bash_task_1 manual__2024-11-28T09:46:48.352196+00:00 [scheduled]>[0m
[[34m2024-11-28T09:46:50.984+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG two_task_dag has 0/16 running and queued tasks[0m
[[34m2024-11-28T09:46:50.984+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: two_task_dag.bash_task_1 manual__2024-11-28T09:46:48.352196+00:00 [scheduled]>[0m
[[34m2024-11-28T09:46:50.986+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='two_task_dag', task_id='bash_task_1', run_id='manual__2024-11-28T09:46:48.352196+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-11-28T09:46:50.986+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'two_task_dag', 'bash_task_1', 'manual__2024-11-28T09:46:48.352196+00:00', '--local', '--subdir', 'DAGS_FOLDER/two_task_dag.py'][0m
[[34m2024-11-28T09:46:50.988+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'two_task_dag', 'bash_task_1', 'manual__2024-11-28T09:46:48.352196+00:00', '--local', '--subdir', 'DAGS_FOLDER/two_task_dag.py'][0m
[[34m2024-11-28T09:46:51.765+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/two_task_dag.py[0m
[[34m2024-11-28T09:46:52.214+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: two_task_dag.bash_task_1 manual__2024-11-28T09:46:48.352196+00:00 [queued]> on host codespaces-c5d7e2[0m
[[34m2024-11-28T09:46:57.652+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='two_task_dag', task_id='bash_task_1', run_id='manual__2024-11-28T09:46:48.352196+00:00', try_number=1, map_index=-1)[0m
[[34m2024-11-28T09:46:57.656+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=two_task_dag, task_id=bash_task_1, run_id=manual__2024-11-28T09:46:48.352196+00:00, map_index=-1, run_start_date=2024-11-28 09:46:52.248957+00:00, run_end_date=2024-11-28 09:46:57.354646+00:00, run_duration=5.105689, state=success, executor_state=success, try_number=1, max_tries=0, job_id=5, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-11-28 09:46:50.985090+00:00, queued_by_job_id=2, pid=11054[0m
[[34m2024-11-28T09:46:57.696+0000[0m] {[34mdagrun.py:[0m630} INFO[0m - Marking run <DagRun two_task_dag @ 2024-11-28 09:46:48.352196+00:00: manual__2024-11-28T09:46:48.352196+00:00, state:running, queued_at: 2024-11-28 09:46:48.363795+00:00. externally triggered: True> successful[0m
[[34m2024-11-28T09:46:57.696+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=two_task_dag, execution_date=2024-11-28 09:46:48.352196+00:00, run_id=manual__2024-11-28T09:46:48.352196+00:00, run_start_date=2024-11-28 09:46:49.183337+00:00, run_end_date=2024-11-28 09:46:57.696621+00:00, run_duration=8.513284, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-11-28 09:46:48.352196+00:00, data_interval_end=2024-11-28 09:46:48.352196+00:00, dag_hash=a09d6de304709dd5ea129fc4a5b66bc1[0m
[[34m2024-11-28T09:47:17.112+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-11-28T09:52:17.152+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-11-28T09:57:17.180+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[2024-11-28T10:01:21.687+0000] {manager.py:542} INFO - DAG extract_dag is missing and will be deactivated.
[2024-11-28T10:01:21.691+0000] {manager.py:552} INFO - Deactivated 1 DAGs which are no longer present in file.
[2024-11-28T10:01:21.707+0000] {manager.py:556} INFO - Deleted DAG extract_dag in serialized_dag table
[[34m2024-11-28T10:02:17.209+0000[0m] {[34mscheduler_job_runner.py:[0m1553} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-11-28T10:03:31.906+0000[0m] {[34mscheduler_job_runner.py:[0m411} INFO[0m - 1 tasks up for execution:
	<TaskInstance: extract_dag.extract_task manual__2024-11-28T10:03:30.152092+00:00 [scheduled]>[0m
[[34m2024-11-28T10:03:31.906+0000[0m] {[34mscheduler_job_runner.py:[0m476} INFO[0m - DAG extract_dag has 0/16 running and queued tasks[0m
[[34m2024-11-28T10:03:31.907+0000[0m] {[34mscheduler_job_runner.py:[0m587} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: extract_dag.extract_task manual__2024-11-28T10:03:30.152092+00:00 [scheduled]>[0m
[[34m2024-11-28T10:03:31.908+0000[0m] {[34mscheduler_job_runner.py:[0m625} INFO[0m - Sending TaskInstanceKey(dag_id='extract_dag', task_id='extract_task', run_id='manual__2024-11-28T10:03:30.152092+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-11-28T10:03:31.908+0000[0m] {[34mbase_executor.py:[0m147} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'extract_dag', 'extract_task', 'manual__2024-11-28T10:03:30.152092+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_dag.py'][0m
[[34m2024-11-28T10:03:31.911+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'extract_dag', 'extract_task', 'manual__2024-11-28T10:03:30.152092+00:00', '--local', '--subdir', 'DAGS_FOLDER/extract_dag.py'][0m
[[34m2024-11-28T10:03:32.681+0000[0m] {[34mdagbag.py:[0m541} INFO[0m - Filling up the DagBag from /workspaces/hands-on-introduction-data-engineering-4395021/airflow/dags/extract_dag.py[0m
[[34m2024-11-28T10:03:33.159+0000[0m] {[34mtask_command.py:[0m410} INFO[0m - Running <TaskInstance: extract_dag.extract_task manual__2024-11-28T10:03:30.152092+00:00 [queued]> on host codespaces-c5d7e2[0m
[[34m2024-11-28T10:03:33.912+0000[0m] {[34mscheduler_job_runner.py:[0m677} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='extract_dag', task_id='extract_task', run_id='manual__2024-11-28T10:03:30.152092+00:00', try_number=1, map_index=-1)[0m
[[34m2024-11-28T10:03:33.916+0000[0m] {[34mscheduler_job_runner.py:[0m713} INFO[0m - TaskInstance Finished: dag_id=extract_dag, task_id=extract_task, run_id=manual__2024-11-28T10:03:30.152092+00:00, map_index=-1, run_start_date=2024-11-28 10:03:33.197133+00:00, run_end_date=2024-11-28 10:03:33.663534+00:00, run_duration=0.466401, state=success, executor_state=success, try_number=1, max_tries=0, job_id=6, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator, queued_dttm=2024-11-28 10:03:31.907461+00:00, queued_by_job_id=2, pid=17603[0m
[[34m2024-11-28T10:03:33.950+0000[0m] {[34mdagrun.py:[0m630} INFO[0m - Marking run <DagRun extract_dag @ 2024-11-28 10:03:30.152092+00:00: manual__2024-11-28T10:03:30.152092+00:00, state:running, queued_at: 2024-11-28 10:03:30.164288+00:00. externally triggered: True> successful[0m
[[34m2024-11-28T10:03:33.950+0000[0m] {[34mdagrun.py:[0m681} INFO[0m - DagRun Finished: dag_id=extract_dag, execution_date=2024-11-28 10:03:30.152092+00:00, run_id=manual__2024-11-28T10:03:30.152092+00:00, run_start_date=2024-11-28 10:03:31.890568+00:00, run_end_date=2024-11-28 10:03:33.950382+00:00, run_duration=2.059814, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-11-28 10:03:30.152092+00:00, data_interval_end=2024-11-28 10:03:30.152092+00:00, dag_hash=5697037627a38bfc911fe854a7265565[0m
